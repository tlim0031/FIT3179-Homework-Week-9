Results = c("Accuracy", "AUC"),
Before = c(bagging_accuracy, auc_bagging),
After = c(bag_remove_accuracy, auc_bag_remove),
# Results = c("Before", "After"),
# Accuracy = c(bagging_accuracy, auc_bagging),
# AUC = c(bag_remove_accuracy, auc_bag_remove),
Difference = c(bag_acc_diff, bag_auc_diff)
)
bag_diff
set.seed(9999)
bag_remove.fit <- bagging(Class ~ . - A03 - A05 - A07 - A10 - A11 - A14 - A21 - A25, data = PD_clean.train)
bag_remove.predict <- predict(bag_remove.fit, PD_clean.test, type="class")
bag_remove_matrix <- table(actual = PD_clean.test$Class, predict = bag_remove.predict$class)
# calculate accuracy
bag_remove_accuracy <- sum(diag(bag_remove_matrix)) / sum(bag_remove_matrix)
# Calculate AUC
bag_remove_conf.predict <- predict(bag_remove.fit, PD_clean.test, type="prob")
conf_bag_remove <- bag_remove_conf.predict$prob[, 2]
pred_bag_remove <- prediction(conf_bag_remove, PD_clean.test$Class)
auc_perf_bag_remove <- performance(pred_bag_remove, "auc")
auc_bag_remove <- as.numeric(auc_perf_bag_remove@y.values)
bag_acc_diff <- bag_remove_accuracy - bagging_accuracy
bag_auc_diff <- auc_bag_remove - auc_bagging
# Create a table to compare the accuracy and AUC value before and after omitting some variables from bagging model
bag_diff <- data.frame(
Results = c("Accuracy", "AUC"),
Before = c(bagging_accuracy, auc_bagging),
After = c(bag_remove_accuracy, auc_bag_remove),
Difference = c(bag_acc_diff, bag_auc_diff)
)
bag_diff
set.seed(9999)
boost_remove.fit <- boosting(Class ~ . -A03 - A07 - A13 - A21 - A25, data = PD_clean.train)
boost_remove.predict <- predict(boost_remove.fit, PD_clean.test, type="class")
boost_remove_matrix <- table(actual = PD_clean.test$Class, predict = boost_remove.predict$class)
# calculate accuracy
boost_remove_accuracy <- sum(diag(boost_remove_matrix)) / sum(boost_remove_matrix)
# Calculate AUC
boost_remove_conf.predict <- predict(boost_remove.fit, PD_clean.test, type="prob")
conf_boost_remove <- boost_remove_conf.predict$prob[, 2]
pred_boost_remove <- prediction(conf_boost_remove, PD_clean.test$Class)
auc_perf_boost_remove <- performance(pred_boost_remove, "auc")
auc_boost_remove <- as.numeric(auc_perf_boost_remove@y.values)
boost_acc_diff <- boost_remove_accuracy - boosting_accuracy
boost_auc_diff <- auc_boost_remove - auc_boosting
# Create a table to compare the accuracy and AUC value before and after omitting some variables from boosting model
boost_diff <- data.frame(
Results = c("Accuracy", "AUC"),
Before = c(boosting_accuracy, auc_boosting),
After = c(boost_remove_accuracy, auc_boost_remove),
Difference = c(boost_acc_diff, boost_auc_diff)
)
boost_diff
set.seed(9999)
rf_remove.fit <- randomForest(Class ~ . -A03, data = PD_clean.train)
rf_remove.predict <- predict(rf_remove.fit, PD_clean.test, type="class")
rf_remove_matrix <- table(actual = PD_clean.test$Class, predict = rf_remove.predict)
# calculate accuracy
rf_remove_accuracy <- sum(diag(rf_remove_matrix)) / sum(rf_remove_matrix)
# Calculate AUC
rf_remove_conf.predict <- predict(rf_remove.fit, PD_clean.test, type="prob")
conf_rf_remove <- rf_remove_conf.predict$prob[, 2]
conf_rf_remove <- rf_remove_conf.predict$prob[,"1"]
conf_rf_remove <- rf_remove_conf.predict[,"1"]
pred_rf_remove <- prediction(conf_rf_remove, PD_clean.test$Class)
auc_perf_rf_remove <- performance(pred_rf_remove, "auc")
auc_rf_remove <- as.numeric(auc_perf_rf_remove@y.values)
rf_acc_diff <- rf_remove_accuracy - random_forest_accuracy
rf_auc_diff <- auc_rf_remove - auc_random_forest
# Create a table to compare the accuracy and AUC value before and after omitting some variables from boosting model
rf_diff <- data.frame(
Results = c("Accuracy", "AUC"),
Before = c(random_forest_accuracy, auc_random_forest),
After = c(rf_remove_accuracy, auc_rf_remove),
Difference = c(rf_acc_diff, rf_auc_diff)
)
rf_diff
knitr::opts_chunk$set(echo = TRUE,
font.family = 'Arial',
size = 12)
knitr::opts_chunk$set(echo = TRUE,
font.family = 'Arial',
size = 12)
getwd()
setwd("C:/Users/Lim Tong En/Documents/Monash/Sem 1 2024/FIT3152/Assignment/A3")
# clean the environment
rm(list = ls())
# load libraries
# install.packages("tm")
# install.packages("tm")
# install.packages("SnowballC")
library(slam)
library(tm)
library(SnowballC)
# get file path to the folder where the documents are located
path_name = file.path(".", "A3 documents")
path_name
# get the contents of the directory
dir(path_name)
# create corpus of documents
corp_docs <- Corpus(DirSource(path_name))
summary(corp_docs)
corp_docs <- tm_map(corp_docs, removeNumbers) # removing numbers
corp_docs <- tm_map(corp_docs, removePunctuation) # removing punctuation
# standardise by converting characters to lower case
corp_docs <- tm_map(corp_docs, content_transformer(tolower))
# perform specific text transformation
# replace original words with their corresponding abbreviations
# replace 'venture capitalist' with vc
toVC <- content_transformer(function(x, pattern)gsub(pattern, "vc", x))
corp_docs <- tm_map(corp_docs, toVC, "venture capitalist")
# replace 'artificial intelligence' with its abbreviation, ai
toAI <- content_transformer(function(x, pattern)gsub(pattern, "ai", x))
corp_docs <- tm_map(corp_docs, toAI, "artificial intelligence")
# replace 'convolutional neural network' with its abbreviation, cnn
toCNN <- content_transformer(function(x, pattern)gsub(pattern, "cnn", x))
corp_docs <- tm_map(corp_docs, toCNN, "convolutional neural network")
# remove common words that are no significant meaning
corp_docs <- tm_map(corp_docs, removeWords, stopwords("english"))
# remove any extra white space
corp_docs <- tm_map(corp_docs, stripWhitespace)
corp_docs <- tm_map(corp_docs, stemDocument, language = "english")
# create document-term matrix
doctm <- DocumentTermMatrix(corp_docs)
inspect(doctm)
# get the size of the original document-term matrix
dim(doctm) # 15 document and 977 terms (tokens)
# Removing columns with 60%
doctms <- removeSparseTerms(doctm, 0.63)
# get the size of the document-term matrix
dim(doctms) # 15 document and 20 terms (tokens)
inspect(doctms)
# create document-term matrix
doctm <- DocumentTermMatrix(corp_docs)
inspect(doctm)
# get the size of the original document-term matrix
dim(doctm) # 15 document and 977 terms (tokens)
# Removing columns with 63%
doctms <- removeSparseTerms(doctm, 0.60)
# get the size of the document-term matrix
dim(doctms) # 15 document and 20 terms (tokens)
inspect(doctms)
# load libraries to perform hierarchical clustering based on cosine distance
#install.packages("proxy")
library(proxy)
doctms_mat <- as.matrix(doctms)
distmatrix <- proxy::dist(doctms_mat, method = "cosine")
# perform hierarchical clustering
h_fit <- hclust(distmatrix, method = "ward.D")
# create a dendogram
plot(h_fit, hang = -1, main = "Document Cosine Distance", xlab = "Documents")
# Cut dendrogram to identify clusters
# Supposed to have 3 clusters in total because there are three different topics
# (entrepreneurship, environment, artificial intelligence)
clusters <- cutree(h_fit, k = 3)
clusters
# Actual Clusters:
# Cluster 1: AI_01, AI_02, AI_03, AI_04, AI_05
# Cluster 2: ENV_01, ENV_02, ENV_03, ENV_04, ENV_05
# Cluster 3: ENTRE_01, ENTRE_02, ENTRE_03, ENTRE_04, ENTRE_05
actual_cluster <- c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2)
# Calculate accuracy of clustering
result <- clusters == actual_cluster
clustering_table <- as.data.frame(clusters)
clustering_table <- cbind(clustering_table, actual_cluster, result)
clustering_table
# calculate accuracy
accuracy <- sum(clustering_table$result == TRUE) / length(clusters) * 100
print(paste("The accuracy of hierarchical clustering is", accuracy, "%"))
# create document-term matrix
doctm <- DocumentTermMatrix(corp_docs)
inspect(doctm)
# get the size of the original document-term matrix
dim(doctm) # 15 document and 977 terms (tokens)
# Removing columns with 63%
doctms <- removeSparseTerms(doctm, 0.63)
# get the size of the document-term matrix
dim(doctms) # 15 document and 20 terms (tokens)
inspect(doctms)
# load libraries to perform hierarchical clustering based on cosine distance
#install.packages("proxy")
library(proxy)
doctms_mat <- as.matrix(doctms)
distmatrix <- proxy::dist(doctms_mat, method = "cosine")
# perform hierarchical clustering
h_fit <- hclust(distmatrix, method = "ward.D")
# create a dendogram
plot(h_fit, hang = -1, main = "Document Cosine Distance", xlab = "Documents")
# Cut dendrogram to identify clusters
# Supposed to have 3 clusters in total because there are three different topics
# (entrepreneurship, environment, artificial intelligence)
clusters <- cutree(h_fit, k = 3)
clusters
# Actual Clusters:
# Cluster 1: AI_01, AI_02, AI_03, AI_04, AI_05
# Cluster 2: ENV_01, ENV_02, ENV_03, ENV_04, ENV_05
# Cluster 3: ENTRE_01, ENTRE_02, ENTRE_03, ENTRE_04, ENTRE_05
actual_cluster <- c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2)
# Calculate accuracy of clustering
result <- clusters == actual_cluster
clustering_table <- as.data.frame(clusters)
clustering_table <- cbind(clustering_table, actual_cluster, result)
clustering_table
# calculate accuracy
accuracy <- sum(clustering_table$result == TRUE) / length(clusters) * 100
print(paste("The accuracy of hierarchical clustering is", accuracy, "%"))
inspect(doctm)
# get the size of the original document-term matrix
dim(doctm) # 15 document and 977 terms (tokens)
# Removing columns with 63%
doctms <- removeSparseTerms(doctm, 0.63)
# get the size of the document-term matrix
dim(doctms) # 15 document and 20 terms (tokens)
inspect(doctms)
doctms_df <- as.data.frame(as.matrix(doctms))
doctms_df
View(doctms_df)
# load libraries to perform hierarchical clustering based on cosine distance
#install.packages("proxy")
library(proxy)
doctms_mat <- as.matrix(doctms)
distmatrix <- proxy::dist(doctms_mat, method = "cosine")
# perform hierarchical clustering
h_fit <- hclust(distmatrix, method = "ward.D")
# create a dendogram
plot(h_fit, hang = -1, main = "Document Cosine Distance", xlab = "Documents")
# Cut dendrogram to identify clusters
# Supposed to have 3 clusters in total because there are three different topics
# (entrepreneurship, environment, artificial intelligence)
clusters <- cutree(h_fit, k = 3)
clusters
clusters
clusters
clusters
# Actual Clusters:
# Cluster 1: AI_01, AI_02, AI_03, AI_04, AI_05
# Cluster 2: ENV_01, ENV_02, ENV_03, ENV_04, ENV_05
# Cluster 3: ENTRE_01, ENTRE_02, ENTRE_03, ENTRE_04, ENTRE_05
actual_cluster <- c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2)
# Calculate accuracy of clustering
result <- clusters == actual_cluster
clustering_table <- as.data.frame(clusters)
clustering_table <- cbind(clustering_table, actual_cluster, result)
clustering_table
clustering_table
# calculate accuracy
accuracy <- sum(clustering_table$result == TRUE) / length(clusters) * 100
print(paste("The accuracy of hierarchical clustering is", accuracy, "%"))
#install.packages(c("igraph", "igraphdata"))
library(igraph)
library(igraphdata)
# convert matrix to binary matrix where entries greater than 0 in original matrix are represented as 1
doctms_bimat <- as.matrix((doctms_mat > 0) + 0)
# multiple the matrix with its transpose
doc_mat <- doctms_bimat %*% t(doctms_bimat)
# make diagonal zero
diag(doc_mat) = 0
# create graph object from the matrix
doc_con <- graph_from_adjacency_matrix(doc_mat, mode = "undirected", weighted = TRUE)
# plot the network graph for the documents
plot(doc_con)
# plot the network graph for the documents
plot(doc_con)
degree_doc <- degree(doc_con)
degree_doc
between_doc <- format(betweenness(doc_con), digits = 3)
between_doc
closeness_doc <- format(closeness(doc_con), digits = 3)
closeness_doc
# identify community clusters using cluster_fast_greedy()
set.seed(32905165)
communities_doc <- cluster_fast_greedy(doc_con)
# plot enhanced network graph
plot(communities_doc, doc_con,
vertex.size = degree_doc,
edge.width = E(doc_con)$weight,
edge.color = "black")
# identify community clusters using cluster_fast_greedy()
set.seed(32905165)
communities_doc <- cluster_fast_greedy(doc_con)
# plot enhanced network graph
plot(communities_doc, doc_con,
vertex.size = degree_doc,
edge.width = E(doc_con)$weight,
edge.color = "black")
# identify community clusters using cluster_fast_greedy()
set.seed(32905165)
communities_doc <- cluster_fast_greedy(doc_con)
# plot enhanced network graph
plot(communities_doc, doc_con,
vertex.size = degree_doc,
edge.width = E(doc_con)$weight * 2,
edge.color = "black")
# identify community clusters using cluster_fast_greedy()
set.seed(32905165)
communities_doc <- cluster_fast_greedy(doc_con)
# plot enhanced network graph
plot(communities_doc, doc_con,
vertex.size = degree_doc,
edge.width = E(doc_con)$weight * 1.5,
edge.color = "black")
# convert matrix to binary matrix where entries greater than 0 in original matrix are represented as 1
doctms_bimat <- as.matrix((doctms_mat > 0) + 0)
# multiple the transpose of matrix with the matrix itself
token_mat <- t(doctms_bimat) %*% doctms_bimat
# make diagonal zero
diag(token_mat) = 0
# create graph object from the matrix
token_con <- graph_from_adjacency_matrix(token_mat, mode = "undirected", weighted = TRUE)
# plot the network graph for tokens
plot(token_con)
degree_token <- degree(token_con)
degree_token
between_token <- format(betweenness(token_con), digits = 3)
between_token
closeness_token <- format(closeness(token_con), digits = 3)
closeness_token
eigen_token <- format(eigen_centrality(token_con)$vector, digits = 3)
eigen_token
# identify community clusters using cluster_fast_greedy()
set.seed(32905165)
communities_token <- cluster_fast_greedy(token_con)
# plot enhanced network graph
plot(communities_token, token_con,
vertex.size = degree_token,
edge.width = E(token_con)$weight,
edge.color = "black")
# convert dtm to data frame
doctms_df <- as.data.frame(as.matrix(doctms))
doctms_df$doc <- rownames(doctms_df)
doctms_bipartite <- data.frame()
for (i in 1:nrow(doctms_df)) {
for (j in 1:(ncol(doctms_df)-1)) {
touse <- cbind(doctms_df[i,j], doctms_df[i, ncol(doctms_df)], colnames(doctms_df[j]))
doctms_bipartite <- rbind(doctms_bipartite, touse)
}
}
# rename columns
colnames(doctms_bipartite) <- c("weight", "abs", "token")
# delete rows with 0 weights
doctms_bipartite_clean <- doctms_bipartite[doctms_bipartite$weight != 0,]
# rearrange columns
doctms_bipartite_clean <- doctms_bipartite_clean[, c(2, 3, 1)]
# create graph object and declare bipartite
bi_g <- graph.data.frame(doctms_bipartite_clean, directed = FALSE)
set.seed(32905165)
V(bi_g)$type <- bipartite.mapping(bi_g)$type
V(bi_g)$color <- ifelse(V(bi_g)$type, "lightblue", "lightpink")
V(bi_g)$shape <- ifelse(V(bi_g)$type, "circle", "square")
E(bi_g)$color <- "black"
plot(bi_g, main = "Bipartite network for corpus")
set.seed(32905165)
clus <- cluster_fast_greedy(bi_g)
plot(clus, bi_g,
vertex.label.color = "black",
edge.width = E(bi_g)$weight,
main = "Enhanced Bipartite Graph for Corpus")
# Cut dendrogram to identify clusters
# Supposed to have 3 clusters in total because there are three different topics
# (entrepreneurship, environment, artificial intelligence)
clusters <- cutree(h_fit, k = 3)
clusters
# Actual Clusters:
# Cluster 1: AI_01, AI_02, AI_03, AI_04, AI_05
# Cluster 2: ENV_01, ENV_02, ENV_03, ENV_04, ENV_05
# Cluster 3: ENTRE_01, ENTRE_02, ENTRE_03, ENTRE_04, ENTRE_05
actual_cluster <- c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2)
# Calculate accuracy of clustering
result <- clusters == actual_cluster
clustering_table <- as.data.frame(clusters)
clustering_table <- cbind(clustering_table, actual_cluster, result)
clustering_table
# calculate accuracy
accuracy <- sum(clustering_table$result == TRUE) / length(clusters) * 100
print(paste("The accuracy of hierarchical clustering is", accuracy, "%"))
View(bi_g)
View(doctms_bipartite_clean)
View(doctms_bipartite)
E(bi_g)
plot(bi_g, main = "Bipartite network for corpus")
# plot the network graph for the documents
plot(doc_con)
#install.packages(c("igraph", "igraphdata"))
library(igraph)
library(igraphdata)
# convert matrix to binary matrix where entries greater than 0 in original matrix are represented as 1
doctms_bimat <- as.matrix((doctms_mat > 0) + 0)
# multiply the matrix with its transpose
doc_mat <- doctms_bimat %*% t(doctms_bimat)
# make diagonal zero
diag(doc_mat) = 0
# create graph object from the matrix
doc_con <- graph_from_adjacency_matrix(doc_mat, mode = "undirected", weighted = TRUE)
# plot the network graph for the documents
plot(doc_con)
# convert matrix to binary matrix where entries greater than 0 in original matrix are represented as 1
doctms_bimat <- as.matrix((doctms_mat > 0) + 0)
# multiply the transpose of matrix with the matrix itself
token_mat <- t(doctms_bimat) %*% doctms_bimat
# make diagonal zero
diag(token_mat) = 0
# create graph object from the matrix
token_con <- graph_from_adjacency_matrix(token_mat, mode = "undirected", weighted = TRUE)
# plot the network graph for tokens
plot(token_con)
# get directory
getwd()
setwd("C:/Users/Lim Tong En/Documents/Monash/Sem 2 2024/FIT3179/Week 9/Homework Week 9/w9 homework/FIT3179-Homework-Week-9")
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv")
world_population <- read.csv("world_population.csv", fileEncoding = "UTF-8")
world_population = read.csv("world_population.csv", fileEncoding = "UTF-8")
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv", fileEncoding = "UTF-8")
world_population = read.csv("world_population.csv")
world_population = read.csv("world_population.csv")
world_population = read.csv("world_population.csv", fileEncoding = "UTF-8")
world_population = read.csv("world_population.csv", fileEncoding = "UTF-8")
# get directory
getwd()
setwd("C:/Users/Lim Tong En/Documents/Monash/Sem 2 2024/FIT3179/Week 9/Homework Week 9/w9 homework/FIT3179-Homework-Week-9")
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv", fileEncoding = "UTF-8")
world_population = read.csv("world_population.csv")
# get directory
getwd()
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv")
View(world_population)
View(world_population)
View(world_tourism_ranking)
View(world_tourism_ranking)
# join the datasets and save into new csv file
combined_dataset <- left_join(world_tourism_ranking, world_population, by = "Country")
# import libraries
library(dplyr)
# join the datasets and save into new csv file
combined_dataset <- left_join(world_tourism_ranking, world_population, by = "Country")
write.csv(combined_data, file = "world_tourism_and_population.csv", row.names = FALSE)
write.csv(combined_dataset, file = "world_tourism_and_population.csv", row.names = FALSE)
# get directory
getwd()
# load datasets
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv")
# convert the 'number of international arrivals' column to standardized format
processed_arrivals <- function(arrivals) {
# Remove whitespace and convert to uppercase
arrivals <- toupper(trimws(arrivals))
# Convert based on the suffix (M for million, K for thousand)
if (grepl("M$", arrivals)) {
return(as.numeric(gsub("M$", "", arrivals)) * 1e6)  # Millions
} else if (grepl("K$", arrivals)) {
return(as.numeric(gsub("K$", "", arrivals)) * 1e3)  # Thousands
} else {
return(as.numeric(arrivals))  # Already numeric or unknown format
}
}
world_tourism_ranking <- world_tourism_ranking %>%
mutate(Annual.International.Tourist.Arrivals = sapply(Annual.International.Tourist.Arrivals, processed_arrivals))
View(world_tourism_ranking)
View(world_tourism_ranking)
# join the datasets and save into new csv file
combined_dataset <- left_join(world_tourism_ranking, world_population, by = "Country")
write.csv(combined_dataset, file = "world_tourism_and_population.csv", row.names = FALSE)
write.csv(combined_dataset, file = "world_tourism_and_population.csv", row.names = FALSE)
View(combined_dataset)
world_tourism_ranking <- world_tourism_ranking %>%
mutate(Annual.International.Tourist.Arrivals = format(sapply(Annual.International.Tourist.Arrivals, processed_arrivals), big.mark = ",", scientific = FALSE))
# join the datasets and save into new csv file
combined_dataset <- left_join(world_tourism_ranking, world_population, by = "Country")
write.csv(combined_dataset, file = "world_tourism_and_population.csv", row.names = FALSE)
# convert the 'number of international arrivals' column to standardized format
processed_arrivals <- function(arrivals) {
# Remove whitespace and convert letters to uppercase
arrivals <- toupper(trimws(arrivals))
# Convert values based on suffix (M: million, K: thousand)
if (grepl("M$", arrivals)) {
return(as.numeric(gsub("M$", "", arrivals)) * 1e6)  # million
}
else if (grepl("K$", arrivals)) {
return(as.numeric(gsub("K$", "", arrivals)) * 1e3)  # thousands
}
else {
return(as.numeric(arrivals))  # already in the correct format
}
}
world_tourism_ranking <- world_tourism_ranking %>%
mutate(Annual.International.Tourist.Arrivals = sapply(Annual.International.Tourist.Arrivals, processed_arrivals))
# load datasets
world_tourism_ranking = read.csv("world_tourism_ranking.csv")
world_population = read.csv("world_population.csv")
# convert the 'number of international arrivals' column to standardized format
processed_arrivals <- function(arrivals) {
# Remove whitespace and convert letters to uppercase
arrivals <- toupper(trimws(arrivals))
# Convert values based on suffix (M: million, K: thousand)
if (grepl("M$", arrivals)) {
return(as.numeric(gsub("M$", "", arrivals)) * 1e6)  # million
}
else if (grepl("K$", arrivals)) {
return(as.numeric(gsub("K$", "", arrivals)) * 1e3)  # thousands
}
else {
return(as.numeric(arrivals))  # already in the correct format
}
}
world_tourism_ranking <- world_tourism_ranking %>%
mutate(Annual.International.Tourist.Arrivals = sapply(Annual.International.Tourist.Arrivals, processed_arrivals))
# join the datasets and save into new csv file
combined_dataset <- left_join(world_tourism_ranking, world_population, by = "Country")
write.csv(combined_dataset, file = "world_tourism_and_population.csv", row.names = FALSE)
